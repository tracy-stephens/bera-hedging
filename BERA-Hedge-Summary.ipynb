{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ccxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperliquid = ccxt.hyperliquid()\n",
    "markets = hyperliquid.load_markets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = {\n",
    "    'swap': True,\n",
    "    'active': True,\n",
    "}\n",
    "\n",
    "volume_limit = 100\n",
    "\n",
    "perp_markets = []\n",
    "usd_volume_1d = {}\n",
    "open_interest = {}\n",
    "mid_px = {}\n",
    "max_leverage = {}\n",
    "for symbol, details in markets.items():\n",
    "    fits_filters = True\n",
    "    for k, v in filters.items():\n",
    "        if details.get(k) != v:\n",
    "            fits_filters = False\n",
    "            break\n",
    "    if fits_filters:\n",
    "        perp_markets.append({symbol: details})\n",
    "        usd_volume_1d[symbol.replace('/USDC:USDC', '')] = details['info']['dayNtlVlm']\n",
    "        open_interest[symbol.replace('/USDC:USDC', '')] = details['info']['openInterest']\n",
    "        mid_px[symbol.replace('/USDC:USDC', '')] = details['info']['midPx']\n",
    "        max_leverage[symbol.replace('/USDC:USDC', '')] = details['info']['maxLeverage']\n",
    "\n",
    "liquidity_df = pd.DataFrame({\n",
    "    'usd_volume_1d': usd_volume_1d,\n",
    "    'open_interest': open_interest,\n",
    "    'mid_px': mid_px,  \n",
    "    'max_leverage': max_leverage,\n",
    "}).astype(float).sort_values(by='usd_volume_1d', ascending=False)\n",
    "\n",
    "liquidity_df['usd_volume_1d'] = liquidity_df['usd_volume_1d'] / 1_000_000\n",
    "liquidity_df['open_interest_usd'] = liquidity_df['open_interest'] * liquidity_df['mid_px'] / 1_000_000\n",
    "#liquidity_df.sort_values(by='open_interest_usd', ascending=False, inplace=True)\n",
    "#liquidity_df.head(30) / 1_000_000\n",
    "\n",
    "volume_thresh = 5 \n",
    "open_int_thresh = 10\n",
    "\n",
    "symbols = liquidity_df.query(\n",
    "    f'open_interest_usd > {open_int_thresh} and usd_volume_1d > {volume_thresh} and max_leverage >= 5'\n",
    ").index.tolist()\n",
    "symbols = [symbol for symbol in symbols if symbol != 'BERA']\n",
    "(liquidity_df.loc[symbols])[['usd_volume_1d', 'open_interest_usd', 'max_leverage']].T.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historical_funding_rate(ticker, start=None, end=None):\n",
    "    #hyperliquid.fetchFundingRateHistory(symbol=f\"{ticker}/USDC:USDC\", since=start, limit=500)\n",
    "    all_funding_rates = []\n",
    "    while start < end:\n",
    "        funding_rate_history = hyperliquid.fetchFundingRateHistory(symbol=f\"{ticker}/USDC:USDC\", since=start, limit=500)\n",
    "        if not funding_rate_history:\n",
    "            break\n",
    "        all_funding_rates.extend(funding_rate_history)\n",
    "        start = funding_rate_history[-1]['timestamp'] + 1  # Move start to the next timestamp\n",
    "\n",
    "    result = {entry['timestamp']: entry['fundingRate'] for entry in all_funding_rates}\n",
    "    return result\n",
    "\n",
    "\n",
    "def historical_close_volume(ticker, start=None, end=None):\n",
    "    all_data = []\n",
    "    while start < end:\n",
    "        ohlcv_data = hyperliquid.fetch_ohlcv(symbol=f\"{ticker}/USDC:USDC\", timeframe='1h', since=start, limit=500)\n",
    "        if not ohlcv_data:\n",
    "            break\n",
    "        all_data.extend(ohlcv_data)\n",
    "        start = ohlcv_data[-1][0] + 1  # Move start to the next timestamp\n",
    "\n",
    "    result = {entry[0]: {'close': entry[4], 'volume': entry[5]} for entry in all_data}  # Extract timestamp, close price, and volume\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_historical_data(ticker, start, end):\n",
    "    funding_rate = historical_funding_rate(ticker, start, end)\n",
    "    close_volume = historical_close_volume(ticker, start, end)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'funding': funding_rate,\n",
    "        'close': {timestamp: data['close'] for timestamp, data in close_volume.items()},\n",
    "        'volume': {timestamp: data['volume'] for timestamp, data in close_volume.items()},\n",
    "    }).sort_index()\n",
    "    df.index = pd.to_datetime(df.index, unit='ms', utc=True)\n",
    "    df = df.resample('h').last()    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = int((datetime.now() - timedelta(days=90)).timestamp() * 1000)\n",
    "end = int(datetime.now().timestamp() * 1000)\n",
    "\n",
    "bera_df = get_historical_data('BERA', start, end)\n",
    "bera_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "start = int(bera_df.index[0].timestamp() * 1000)\n",
    "for symbol in tqdm(symbols):\n",
    "    data[symbol] = get_historical_data(symbol, start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_df = pd.DataFrame({k: df['funding'] for k, df in data.items()})\n",
    "price_df = pd.DataFrame({k: df['close'] for k, df in data.items()})\n",
    "volume_df = pd.DataFrame({k: df['volume'] for k, df in data.items()})\n",
    "notional_df = price_df * volume_df\n",
    "\n",
    "funding_df.head()\n",
    "price_df.head()\n",
    "notional_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "ax = (funding_df.mean() * 24 * 365 * 100).sort_values(ascending=False).plot.bar(figsize=(12, 3), title='Annualized Funding Rate')\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: f'{y:.0f}%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '2025-02-15 13:00:00+00:00'\n",
    "\n",
    "bera_returns = bera_df['close'].loc[start:].pct_change().dropna()\n",
    "asset_returns = price_df.loc[start:].pct_change().dropna()\n",
    "bera_returns, asset_returns = bera_returns.align(asset_returns, join='inner', axis=0)\n",
    "\n",
    "mean_returns = asset_returns.mean()\n",
    "cov_matrix = asset_returns.cov()\n",
    "\n",
    "correlations = asset_returns.corrwith(bera_returns)\n",
    "\n",
    "pd.DataFrame({\n",
    "    'corr since 2/15/25': correlations.sort_values(ascending=False)\n",
    "}).T.style.format(\"{:.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "n_components = 5\n",
    "pca = PCA(n_components=n_components)\n",
    "pca_scores = pca.fit_transform(asset_returns)\n",
    "\n",
    "pc1 = pca.transform(asset_returns)[:, 0]\n",
    "\n",
    "explained_variance = []\n",
    "for i in range(n_components):\n",
    "    pc = pca_scores[:, i].reshape(-1, 1)\n",
    "    model = LinearRegression().fit(pc, bera_returns)\n",
    "    r2 = model.score(pc, bera_returns)\n",
    "    explained_variance.append(r2 * 100)  # percent\n",
    "\n",
    "# Step 4: Output results\n",
    "for i, r2 in enumerate(explained_variance, start=1):\n",
    "    print(f\"PC{i} explains {r2:.2f}% of BERA's return variation.\")\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "pc1 = pca.fit_transform(asset_returns).flatten()\n",
    "\n",
    "correlation = np.corrcoef(pc1, bera_returns.values)[0, 1]\n",
    "print(f\"Correlation between BERA returns and market component (PC1): {correlation:.3f}\")\n",
    "\n",
    "pc1_weights = pd.Series(\n",
    "    pca.components_[0],\n",
    "    index=asset_returns.columns,\n",
    "    name='PC1 loading'\n",
    ")\n",
    "pca_market_portfolio = pc1_weights / pc1_weights.abs().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notional_df.mean() * 24 / 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "funding_vector = -(funding_df[asset_returns.columns].mean() * 24 * 365).values\n",
    "\n",
    "pca = PCA(n_components=1)  # you can tune this\n",
    "pca_factors = pca.fit_transform(asset_returns)\n",
    "\n",
    "# Step 2: Reconstruct returns using top components only\n",
    "cleaned_returns = pca.inverse_transform(pca_factors)\n",
    "cleaned_returns_df = pd.DataFrame(cleaned_returns, index=asset_returns.index, columns=asset_returns.columns)\n",
    "\n",
    "# Align indices\n",
    "bera_returns, asset_returns = bera_returns.align(cleaned_returns_df, join='inner', axis=0)\n",
    "\n",
    "# Convert to numpy\n",
    "R = asset_returns.values\n",
    "r_bera = bera_returns.values\n",
    "n_assets = R.shape[1]\n",
    "w = cp.Variable(n_assets)\n",
    "\n",
    "r_portfolio = R @ w\n",
    "\n",
    "tracking_error = cp.sum_squares(r_portfolio - r_bera)\n",
    "funding_penalty = funding_vector @ w  # linear penalty\n",
    "lambda_funding = 0.0001 # you can tune this\n",
    "\n",
    "objective = cp.Minimize(tracking_error + lambda_funding * funding_penalty)\n",
    "\n",
    "constraints = [cp.sum(w) == 1, w >= 0]\n",
    "\n",
    "problem = cp.Problem(objective, constraints)\n",
    "problem.solve()\n",
    "\n",
    "optimal_weights = pd.Series(w.value, index=asset_returns.columns)\n",
    "optimal_weights = pd.Series(w.value, index=asset_returns.columns)\n",
    "filtered_weights = optimal_weights[optimal_weights > 1e-3]  # filter threshold = 0.001\n",
    "\n",
    "pd.DataFrame({\n",
    "    'weights': filtered_weights,\n",
    "    'funding rate (ann.)': -pd.Series(funding_vector, index=asset_returns.columns),\n",
    "}).dropna().sort_values(by='weights', ascending=False).round(3).style.format(\"{:.1%}\")\n",
    "#filtered_weights.sort_values(ascending=False).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate portfolio returns\n",
    "tracked_returns = asset_returns.dot(optimal_weights)\n",
    "\n",
    "tracked_returns.rolling(24*7).corr(bera_returns).dropna().plot(\n",
    "    figsize=(12, 3), \n",
    "    title='7-Day Rolling Correlation with BERA'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate portfolio returns\n",
    "tracked_returns = asset_returns.dot(optimal_weights)\n",
    "\n",
    "# Correlation with BERA\n",
    "correlation = tracked_returns.corr(bera_returns)\n",
    "print(f\"Optimized correlation: {correlation:.5f}\")\n",
    "\n",
    "# Optional: Plot cumulative returns\n",
    "((1 + bera_returns).cumprod() - 1).plot(label=\"BERA\", linewidth=2)\n",
    "((1 + tracked_returns).cumprod() - 1).plot(label=\"Optimized Basket\", linewidth=2)\n",
    "plt.title(\"Tracking Optimization vs. BERA\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_vector = -(funding_df[asset_returns.columns].mean() * 24 * 365).values\n",
    "\n",
    "pca = PCA(n_components=1)  # you can tune this\n",
    "pca_factors = pca.fit_transform(asset_returns)\n",
    "\n",
    "# Step 2: Reconstruct returns using top components only\n",
    "cleaned_returns = pca.inverse_transform(pca_factors)\n",
    "cleaned_returns_df = pd.DataFrame(cleaned_returns, index=asset_returns.index, columns=asset_returns.columns)\n",
    "residual_returns = asset_returns - cleaned_returns_df\n",
    "\n",
    "# Align indices\n",
    "bera_returns, residual_returns = bera_returns.align(residual_returns, join='inner', axis=0)\n",
    "R = residual_returns.values\n",
    "r_bera = bera_returns.values\n",
    "\n",
    "n_assets = R.shape[1]\n",
    "w = cp.Variable(n_assets)\n",
    "\n",
    "r_portfolio = R @ w\n",
    "\n",
    "tracking_error = cp.sum_squares(r_portfolio - r_bera)\n",
    "funding_penalty = funding_vector @ w  # linear penalty\n",
    "lambda_funding = 0.0001 # you can tune this\n",
    "\n",
    "objective = cp.Minimize(tracking_error + lambda_funding * funding_penalty)\n",
    "\n",
    "constraints = [cp.sum(w) == 1, w >= 0]\n",
    "\n",
    "problem = cp.Problem(objective, constraints)\n",
    "problem.solve()\n",
    "\n",
    "optimal_weights = pd.Series(w.value, index=asset_returns.columns)\n",
    "optimal_weights = pd.Series(w.value, index=asset_returns.columns)\n",
    "filtered_weights = optimal_weights[optimal_weights > 1e-3]  # filter threshold = 0.001\n",
    "\n",
    "pd.DataFrame({\n",
    "    'weights': filtered_weights,\n",
    "    'funding rate (ann.)': -pd.Series(funding_vector, index=asset_returns.columns),\n",
    "}).dropna().sort_values(by='weights', ascending=False).round(3).style.format(\"{:.1%}\")\n",
    "#filtered_weights.sort_values(ascending=False).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for volume\n",
    "\n",
    "daily_volume = notional_df.mean() * 24\n",
    "basket_size = 5_000_000\n",
    "max_pct_daily_volume = 0.05\n",
    "\n",
    "# Compute max allowed notional per asset\n",
    "max_allocation_usd = daily_volume * max_pct_daily_volume\n",
    "\n",
    "# Convert to max weights relative to total basket size\n",
    "max_weights = max_allocation_usd / basket_size\n",
    "\n",
    "# Align and convert to numpy\n",
    "max_weights = max_weights[asset_returns.columns].fillna(1.0)  # fallback to 100% if volume missing\n",
    "\n",
    "funding_vector = -(funding_df[asset_returns.columns].mean() * 24 * 365).values\n",
    "\n",
    "pca = PCA(n_components=1)  # you can tune this\n",
    "pca_factors = pca.fit_transform(asset_returns)\n",
    "\n",
    "# Step 2: Reconstruct returns using top components only\n",
    "cleaned_returns = pca.inverse_transform(pca_factors)\n",
    "cleaned_returns_df = pd.DataFrame(cleaned_returns, index=asset_returns.index, columns=asset_returns.columns)\n",
    "\n",
    "# Align indices\n",
    "bera_returns, asset_returns = bera_returns.align(cleaned_returns_df, join='inner', axis=0)\n",
    "\n",
    "# Convert to numpy\n",
    "R = asset_returns.values\n",
    "r_bera = bera_returns.values\n",
    "n_assets = R.shape[1]\n",
    "w = cp.Variable(n_assets)\n",
    "\n",
    "r_portfolio = R @ w\n",
    "\n",
    "tracking_error = cp.sum_squares(r_portfolio - r_bera)\n",
    "funding_penalty = funding_vector @ w  # linear penalty\n",
    "lambda_funding = 0.0001 # you can tune this\n",
    "\n",
    "objective = cp.Minimize(tracking_error + lambda_funding * funding_penalty)\n",
    "\n",
    "constraints = [\n",
    "    cp.sum(w) == 1,\n",
    "    w >= 0,\n",
    "    w <= max_weights.values  # enforce volume constraint\n",
    "]\n",
    "\n",
    "problem = cp.Problem(objective, constraints)\n",
    "problem.solve()\n",
    "\n",
    "optimal_weights = pd.Series(w.value, index=asset_returns.columns)\n",
    "optimal_weights = pd.Series(w.value, index=asset_returns.columns)\n",
    "filtered_weights = optimal_weights[optimal_weights > 1e-3]  # filter threshold = 0.001\n",
    "\n",
    "pd.DataFrame({\n",
    "    'weights': filtered_weights,\n",
    "    'funding rate (ann.)': -pd.Series(funding_vector, index=asset_returns.columns),\n",
    "    'pct_daily_volume ($5m Basket)': (optimal_weights * basket_size) / daily_volume,\n",
    "}).dropna().sort_values(by='weights', ascending=False).round(3).style.format(\"{:.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate portfolio returns\n",
    "tracked_returns = asset_returns.dot(optimal_weights)\n",
    "\n",
    "tracked_returns.rolling(24*7).corr(bera_returns).dropna().plot(\n",
    "    figsize=(12, 3), \n",
    "    title='7-Day Rolling Correlation with BERA'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(pd.to_datetime('2025-03-03 12:00:00+00:00').timestamp())*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basket import HedgeBasket\n",
    "\n",
    "basket_dates = [\n",
    "    '2025-03-03 12:00:00+00:00',\n",
    "    '2025-03-10 12:00:00+00:00',\n",
    "    '2025-03-17 12:00:00+00:00',\n",
    "    '2025-03-24 12:00:00+00:00',\n",
    "    '2025-03-31 12:00:00+00:00',\n",
    "    '2025-04-07 12:00:00+00:00',\n",
    "]\n",
    "\n",
    "baskets = {\n",
    "    dt: HedgeBasket(\n",
    "        ticker='BERA',\n",
    "        timestamp=int(pd.to_datetime(dt).timestamp())*1000,\n",
    "        basket_size=5_000_000,\n",
    "        max_pct_daily_volume=0.05,\n",
    "        lambda_funding=0.1\n",
    "    ) for dt in basket_dates\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {}\n",
    "for dt, basket in tqdm(baskets.items()):\n",
    "    weights[dt] = basket.optimized_weights['weights']\n",
    "    print(f\"The correlation between the basket and BERA for {dt[:10]} is {basket.correlation:.2f}.\")\n",
    "    #print(weights[dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_df = pd.DataFrame(weights)\n",
    "\n",
    "weights_clean = weights_df.fillna(0)\n",
    "weights_clean.columns = pd.to_datetime(weights_df.columns).date\n",
    "weights_clean.sort_values(by=weights_clean.columns[0], ascending=False).style.format(\"{:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'In Sample Correlation to BERA': pd.Series({dt: basket.correlation for dt, basket in baskets.items()}),\n",
    "    'Avg Annualized Funding Rate': pd.Series({\n",
    "        dt: basket.optimized_weights['funding rate (ann.)'].dot(basket.optimized_weights['weights']) for dt, basket in baskets.items()\n",
    "    }),\n",
    "}).style.format(\"{:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "basket_assets = weights_df.index.to_list()\n",
    "\n",
    "basket_asset_data = {}\n",
    "data_start = int(pd.to_datetime(basket_dates[0]).timestamp())*1000\n",
    "data_end = int(datetime.utcnow().timestamp()) * 1000\n",
    "for symbol in tqdm(basket_assets):\n",
    "    basket_asset_data[symbol] = get_historical_data(symbol, data_start, data_end)\n",
    "    time.sleep(2)\n",
    "\n",
    "basket_asset_funding = pd.DataFrame({symbol: data['funding'] for symbol, data in basket_asset_data.items()})\n",
    "basket_asset_close = pd.DataFrame({symbol: data['close'] for symbol, data in basket_asset_data.items()})\n",
    "basket_asset_returns = basket_asset_close.pct_change()\n",
    "basket_asset_volume = pd.DataFrame({symbol: data['volume'] for symbol, data in basket_asset_data.items()})\n",
    "basket_asset_volume_usd = basket_asset_volume * basket_asset_close\n",
    "\n",
    "benchmark_returns = get_historical_data('BERA', data_start, data_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket_asset_funding.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = {}\n",
    "basket_returns = {}\n",
    "volatilty = {}\n",
    "benchmark_volatility = {}\n",
    "for dt, basket in baskets.items():\n",
    "    basket_returns[dt] = (basket.asset_returns[basket.optimized_weights.index] * basket.optimized_weights['weights']).sum(1)\n",
    "    correlation[dt] = basket_returns[dt].corr(basket.benchmark_returns)\n",
    "    volatilty[dt] = basket_returns[dt].std() * np.sqrt(24 * 365)\n",
    "    benchmark_volatility[dt] = basket.benchmark_returns.std() * np.sqrt(24 * 365)\n",
    "    #print(f\"The correlation between the basket and BERA for {dt[:10]} is {correlation[dt]:.2f}.\")\n",
    "\n",
    "pd.DataFrame({\n",
    "    'In Sample Correlation to BERA': pd.Series({dt: correlation[dt] for dt, basket in baskets.items()}),\n",
    "    'Avg Annualized Funding Rate': pd.Series({\n",
    "        dt: basket.optimized_weights['funding rate (ann.)'].dot(basket.optimized_weights['weights']) for dt, basket in baskets.items()\n",
    "    }),\n",
    "    'Basket Volatility': pd.Series(volatilty),\n",
    "    'Benchmark Volatility': pd.Series(benchmark_volatility),\n",
    "}).to_clipboard()#.style.format(\"{:.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pnl_data = pd.read_clipboard(index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 24*7  # 7-day chunks\n",
    "spot_array = (pnl_data['spot_pnl'] / 3_000_000).values\n",
    "hedge_array = (pnl_data['hedge_pnl'] / 3_000_000).values\n",
    "\n",
    "max_start = len(spot_array) - chunk_size + 1  # +1 to include last possible chunk\n",
    "\n",
    "# Rolling chunks\n",
    "spot_chunks = [spot_array[i:i+chunk_size].sum() for i in range(max_start)]\n",
    "hedge_chunks = [hedge_array[i:i+chunk_size].sum() for i in range(max_start)]\n",
    "\n",
    "spot_chunks = np.array(spot_chunks)\n",
    "hedge_chunks = np.array(hedge_chunks)\n",
    "\n",
    "n_sims = 10000\n",
    "chunks_per_year = 52  # using daily chunks\n",
    "\n",
    "spot_annual = []\n",
    "hedge_annual = []\n",
    "combined_annual = []\n",
    "\n",
    "for _ in range(n_sims):\n",
    "    idx = np.random.randint(0, len(spot_chunks), size=chunks_per_year)\n",
    "    spot_sample = spot_chunks[idx]\n",
    "    hedge_sample = hedge_chunks[idx]\n",
    "    \n",
    "    spot_return = np.prod(1 + spot_sample) - 1\n",
    "    hedge_return = np.prod(1 + hedge_sample) - 1\n",
    "    combined_return = np.prod(1 + (spot_sample + hedge_sample)) - 1\n",
    "\n",
    "    spot_annual.append(spot_return)\n",
    "    hedge_annual.append(hedge_return)\n",
    "    combined_annual.append(combined_return)\n",
    "\n",
    "spot_annual = np.array(spot_annual)\n",
    "hedge_annual = np.array(hedge_annual)\n",
    "combined_annual = np.array(combined_annual)\n",
    "\n",
    "# Summary percentiles\n",
    "pct_levels = [.99, .95, .9, .75, .5, .25, .1, .05, .01]\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'percentile': pct_levels,\n",
    "    'combined': np.percentile(combined_annual, [p * 100 for p in pct_levels]),\n",
    "    #$'spot_only': np.percentile(spot_annual, [p * 100 for p in pct_levels]),\n",
    "    #'hedge_only': np.percentile(hedge_annual, [p * 100 for p in pct_levels]),\n",
    "})\n",
    "\n",
    "summary_df.to_clipboard(index=False)\n",
    "summary_df.style.format(\"{:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(spot_chunks, bins=50, alpha=0.5, label='spot')\n",
    "plt.hist(hedge_chunks, bins=50, alpha=0.5, label='hedge')\n",
    "plt.hist(combined_chunks, bins=50, alpha=0.5, label='combined')\n",
    "plt.legend()\n",
    "plt.title(\"Distribution of Weekly Returns\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
