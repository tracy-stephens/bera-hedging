{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ccxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperliquid = ccxt.hyperliquid()\n",
    "markets = hyperliquid.load_markets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = {\n",
    "    'swap': True,\n",
    "    'active': True,\n",
    "}\n",
    "\n",
    "volume_limit = 100\n",
    "\n",
    "perp_markets = []\n",
    "usd_volume_1d = {}\n",
    "open_interest = {}\n",
    "mid_px = {}\n",
    "for symbol, details in markets.items():\n",
    "    fits_filters = True\n",
    "    for k, v in filters.items():\n",
    "        if details.get(k) != v:\n",
    "            fits_filters = False\n",
    "            break\n",
    "    if fits_filters:\n",
    "        perp_markets.append({symbol: details})\n",
    "        usd_volume_1d[symbol.replace('/USDC:USDC', '')] = details['info']['dayNtlVlm']\n",
    "        open_interest[symbol.replace('/USDC:USDC', '')] = details['info']['openInterest']\n",
    "        mid_px[symbol.replace('/USDC:USDC', '')] = details['info']['midPx']\n",
    "\n",
    "liquidity_df = pd.DataFrame({\n",
    "    'usd_volume_1d': usd_volume_1d,\n",
    "    'open_interest': open_interest,\n",
    "    'mid_px': mid_px,  \n",
    "}).astype(float).sort_values(by='usd_volume_1d', ascending=False)\n",
    "\n",
    "liquidity_df['open_interest_usd'] = liquidity_df['open_interest'] * liquidity_df['mid_px']\n",
    "#liquidity_df.sort_values(by='open_interest_usd', ascending=False, inplace=True)\n",
    "#liquidity_df.head(30) / 1_000_000\n",
    "\n",
    "volume_thresh = 5 * 1_000_000\n",
    "open_int_thresh = 10 * 1_000_000\n",
    "\n",
    "symbols = liquidity_df.query(f'open_interest_usd > {open_int_thresh} and usd_volume_1d > {volume_thresh}').index.tolist()\n",
    "symbols = [symbol for symbol in symbols if symbol != 'BERA']\n",
    "liquidity_df.loc[symbols] / 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historical_funding_rate(ticker, start=None, end=None):\n",
    "    #hyperliquid.fetchFundingRateHistory(symbol=f\"{ticker}/USDC:USDC\", since=start, limit=500)\n",
    "    all_funding_rates = []\n",
    "    while start < end:\n",
    "        funding_rate_history = hyperliquid.fetchFundingRateHistory(symbol=f\"{ticker}/USDC:USDC\", since=start, limit=500)\n",
    "        if not funding_rate_history:\n",
    "            break\n",
    "        all_funding_rates.extend(funding_rate_history)\n",
    "        start = funding_rate_history[-1]['timestamp'] + 1  # Move start to the next timestamp\n",
    "\n",
    "    result = {entry['timestamp']: entry['fundingRate'] for entry in all_funding_rates}\n",
    "    return result\n",
    "\n",
    "\n",
    "def historical_close_volume(ticker, start=None, end=None):\n",
    "    all_data = []\n",
    "    while start < end:\n",
    "        ohlcv_data = hyperliquid.fetch_ohlcv(symbol=f\"{ticker}/USDC:USDC\", timeframe='1h', since=start, limit=500)\n",
    "        if not ohlcv_data:\n",
    "            break\n",
    "        all_data.extend(ohlcv_data)\n",
    "        start = ohlcv_data[-1][0] + 1  # Move start to the next timestamp\n",
    "\n",
    "    result = {entry[0]: {'close': entry[4], 'volume': entry[5]} for entry in all_data}  # Extract timestamp, close price, and volume\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_historical_data(ticker, start, end):\n",
    "    funding_rate = historical_funding_rate(ticker, start, end)\n",
    "    close_volume = historical_close_volume(ticker, start, end)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'funding': funding_rate,\n",
    "        'close': {timestamp: data['close'] for timestamp, data in close_volume.items()},\n",
    "        'volume': {timestamp: data['volume'] for timestamp, data in close_volume.items()},\n",
    "    }).sort_index()\n",
    "    df.index = pd.to_datetime(df.index, unit='ms', utc=True)\n",
    "    df = df.resample('h').last()    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = int((datetime.now() - timedelta(days=90)).timestamp() * 1000)\n",
    "end = int(datetime.now().timestamp() * 1000)\n",
    "\n",
    "bera_df = get_historical_data('BERA', start, end)\n",
    "bera_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "start = int(bera_df.index[0].timestamp() * 1000)\n",
    "for symbol in tqdm(symbols):\n",
    "    data[symbol] = get_historical_data(symbol, start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_df = pd.DataFrame({k: df['funding'] for k, df in data.items()})\n",
    "price_df = pd.DataFrame({k: df['close'] for k, df in data.items()})\n",
    "volume_df = pd.DataFrame({k: df['volume'] for k, df in data.items()})\n",
    "notional_df = price_df * volume_df\n",
    "\n",
    "funding_df.head()\n",
    "price_df.head()\n",
    "notional_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '2025-02-15 00:00:00+00:00'\n",
    "\n",
    "bera_returns = bera_df['close'].loc[start:].pct_change().dropna()\n",
    "asset_returns = price_df.loc[start:].pct_change().dropna()\n",
    "bera_returns, asset_returns = bera_returns.align(asset_returns, join='inner', axis=0)\n",
    "\n",
    "mean_returns = asset_returns.mean()\n",
    "cov_matrix = asset_returns.cov()\n",
    "\n",
    "correlations = asset_returns.corrwith(bera_returns)\n",
    "correlations.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "# Number of assets\n",
    "num_assets = asset_returns.shape[1]\n",
    "\n",
    "# Variables\n",
    "weights = cp.Variable(num_assets)\n",
    "\n",
    "# Portfolio return\n",
    "portfolio_return = asset_returns.values @ weights\n",
    "\n",
    "# Tracking error\n",
    "tracking_error = cp.norm(portfolio_return - bera_returns, 'fro')\n",
    "\n",
    "# Objective function: Minimize tracking error\n",
    "objective = cp.Minimize(tracking_error)\n",
    "\n",
    "# Constraints\n",
    "constraints = [\n",
    "    cp.sum(weights) == 1,       # Weights sum to 1\n",
    "    weights >= 0,               # No short selling\n",
    "    weights <= 0.5              # Example: No more than 20% in any asset\n",
    "]\n",
    "\n",
    "# Problem definition\n",
    "problem = cp.Problem(objective, constraints)\n",
    "\n",
    "# Solve the problem\n",
    "problem.solve()\n",
    "\n",
    "# Optimal weights\n",
    "optimal_weights = weights.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_returns = asset_returns.dot(optimal_weights)\n",
    "# Align the indices of portfolio_returns and bera_returns\n",
    "portfolio_returns, bera_returns = portfolio_returns.align(bera_returns, join='inner')\n",
    "# Calculate the differences between portfolio and benchmark returns\n",
    "return_differences = portfolio_returns - bera_returns\n",
    "\n",
    "# Compute the tracking error\n",
    "tracking_error = np.std(return_differences)\n",
    "bera_returns.corr(portfolio_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_names = asset_returns.columns\n",
    "portfolio_weights = pd.Series(data=optimal_weights, index=asset_names, name='Weight')\n",
    "portfolio_weights.sort_values(ascending=False, inplace=True)\n",
    "portfolio_weights.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative returns\n",
    "cumulative_bera_returns = (1 + bera_returns).cumprod() - 1\n",
    "cumulative_portfolio_returns = (1 + portfolio_returns).cumprod() - 1\n",
    "\n",
    "cumulative_bera_returns = bera_returns.cumsum()\n",
    "cumulative_portfolio_returns = portfolio_returns.cumsum()\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(cumulative_bera_returns, label='BERA Asset Cumulative Returns', color='blue')\n",
    "plt.plot(cumulative_portfolio_returns, label='Optimized Portfolio Cumulative Returns', color='orange')\n",
    "plt.title('Cumulative Returns: BERA Asset vs. Optimized Portfolio')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cumulative Return')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '2025-03-09 00:00:00+00:00'\n",
    "\n",
    "position_size = 1_000_000\n",
    "position_size_bera = position_size/ bera_df['close'].loc[start]\n",
    "\n",
    "bera_spot_pnl = (bera_df['close'].loc[start:] - bera_df['close'].loc[start]) * position_size_bera\n",
    "bera_perp_pnl = -1 * (bera_df['close'].loc[start:] - bera_df['close'].loc[start]) * position_size_bera\n",
    "bera_perp_notional = position_size + bera_perp_pnl\n",
    "bera_funding_costs = bera_df['funding'].loc[start:] * (position_size + bera_perp_pnl)\n",
    "\n",
    "portfolio_position_sizes = position_size * portfolio_weights[price_df.columns]\n",
    "portfolio_sizes_quote = portfolio_position_sizes / price_df[portfolio_position_sizes.index].loc[start]\n",
    "\n",
    "portfolio_perp_pnl = (-1 * (price_df.loc[start:] - price_df.loc[start]) * portfolio_sizes_quote)\n",
    "portfolio_funding_costs = funding_df.loc[start:] * (portfolio_position_sizes + portfolio_perp_pnl)\n",
    "\n",
    "pd.DataFrame({\n",
    "    'BERA Perp': bera_spot_pnl + bera_perp_pnl + bera_funding_costs.fillna(0).cumsum(),\n",
    "    'Portfolio Perp': (bera_spot_pnl + portfolio_perp_pnl.sum(1) + portfolio_funding_costs.sum(1).cumsum()).dropna(),\n",
    "}).plot()\n",
    "\n",
    "bera_funding_costs.sum() * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Calculate returns\n",
    "returns_df = price_df.pct_change().dropna()\n",
    "\n",
    "# Standardize the returns\n",
    "scaler = StandardScaler()\n",
    "returns_scaled = scaler.fit_transform(returns_df)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(returns_scaled)\n",
    "\n",
    "# Obtain principal components\n",
    "principal_components = pca.transform(returns_scaled)\n",
    "\n",
    "# Create a DataFrame for principal components\n",
    "pc_df = pd.DataFrame(data=principal_components, index=returns_df.index)\n",
    "\n",
    "# Correlate the first principal component with the target asset's returns\n",
    "target_returns = bera_df['close'].pct_change().dropna()\n",
    "correlation_with_pc1 = pc_df.iloc[:, 0].corr(target_returns)\n",
    "print(f\"Correlation with PC1: {correlation_with_pc1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get loadings (eigenvectors)\n",
    "loadings = pca.components_\n",
    "\n",
    "# Focus on the first principal component\n",
    "pc1_loadings = loadings[0]\n",
    "\n",
    "# Create a DataFrame for asset loadings\n",
    "loadings_df = pd.DataFrame(data=pc1_loadings, index=returns_df.columns, columns=['PC1 Loading'])\n",
    "\n",
    "# Normalize loadings to sum to 1 for weight adjustment\n",
    "loadings_df['Adjusted Weight'] = loadings_df['PC1 Loading'] / loadings_df['PC1 Loading'].sum()\n",
    "\n",
    "# Display adjusted weights\n",
    "print(loadings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply adjusted weights to the asset basket\n",
    "optimized_weights = loadings_df['Adjusted Weight']\n",
    "\n",
    "# Ensure no negative weights (if short selling is not allowed)\n",
    "#optimized_weights = optimized_weights.clip(lower=0)\n",
    "\n",
    "# Normalize weights to sum to 1\n",
    "optimized_weights /= optimized_weights.sum()\n",
    "\n",
    "print(\"Optimized Portfolio Weights:\")\n",
    "print(optimized_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the portfolio returns with the new weights\n",
    "optimized_portfolio_returns = (returns_df * optimized_weights).sum(axis=1)\n",
    "\n",
    "# Correlate the optimized portfolio returns with the target asset's returns\n",
    "optimized_correlation = optimized_portfolio_returns.corr(target_returns)\n",
    "print(f\"Optimized Correlation with Target Asset: {optimized_correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Run PCA on asset returns\n",
    "pca = PCA(n_components=5)  # you can tune this\n",
    "pca_factors = pca.fit_transform(asset_returns)\n",
    "\n",
    "# Step 2: Reconstruct returns using top components only\n",
    "cleaned_returns = pca.inverse_transform(pca_factors)\n",
    "cleaned_returns_df = pd.DataFrame(cleaned_returns, index=asset_returns.index, columns=asset_returns.columns)\n",
    "\n",
    "# Align indices\n",
    "bera_returns, asset_returns = bera_returns.align(cleaned_returns_df, join='inner', axis=0)\n",
    "\n",
    "# Convert to numpy\n",
    "R = asset_returns.values\n",
    "r_bera = bera_returns.values\n",
    "\n",
    "n_assets = R.shape[1]\n",
    "w = cp.Variable(n_assets)\n",
    "\n",
    "# Predicted portfolio return\n",
    "r_portfolio = R @ w\n",
    "\n",
    "# Objective: Minimize squared tracking error\n",
    "objective = cp.Minimize(cp.sum_squares(r_portfolio - r_bera))\n",
    "\n",
    "# Constraints: weights sum to 1, and optional bounds\n",
    "constraints = [cp.sum(w) == 1, w >= 0]\n",
    "\n",
    "# Solve\n",
    "problem = cp.Problem(objective, constraints)\n",
    "problem.solve()\n",
    "\n",
    "# Extract weights\n",
    "optimal_weights = pd.Series(w.value, index=asset_returns.columns)\n",
    "optimal_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate portfolio returns\n",
    "tracked_returns = asset_returns.dot(optimal_weights)\n",
    "\n",
    "# Correlation with BERA\n",
    "correlation = tracked_returns.corr(bera_returns)\n",
    "print(f\"Optimized correlation: {correlation:.5f}\")\n",
    "\n",
    "# Optional: Plot cumulative returns\n",
    "((1 + bera_returns).cumprod() - 1).plot(label=\"BERA\", linewidth=2)\n",
    "((1 + tracked_returns).cumprod() - 1).plot(label=\"Optimized Basket\", linewidth=2)\n",
    "plt.title(\"Tracking Optimization vs. BERA\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
